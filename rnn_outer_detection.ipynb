{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "import os,sys\n",
    "import tqdm\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootPath ='D:/git/IoT_Sensors_Security_Analysis/data/perf/'\n",
    "resultsPath = 'D:/git/IoT_Sensors_Security_Analysis/results/tw_50_turn_1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Exception ignored in: <function _ConnectionBase.__del__ at 0x000001EB2C67E700>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\multiprocessing\\connection.py\", line 137, in __del__\n",
      "QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\multiprocessing\\queues.py\", line 241, in _feed\n",
      "        close()\n",
      "  File \"C:\\Python39\\lib\\multiprocessing\\connection.py\", line 182, in close\n",
      "self._close()\n",
      "  File \"C:\\Python39\\lib\\multiprocessing\\connection.py\", line 282, in _close\n",
      "    self._close()\n",
      "  File \"C:\\Python39\\lib\\multiprocessing\\connection.py\", line 282, in _close\n",
      "    _CloseHandle(self._handle)\n",
      "OSError: [WinError 6] The handle is invalid\n",
      "    _CloseHandle(self._handle)\n",
      "OSError: [WinError 6] The handle is invalid\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Python39\\lib\\threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Python39\\lib\\multiprocessing\\queues.py\", line 272, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n"
     ]
    }
   ],
   "source": [
    "encoded_trace_df = pd.read_pickle(resultsPath+'encoded_bow.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3932.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>126092.336216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11414.148432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>99542.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>117191.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>127012.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>136056.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>143245.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "count    3932.000000\n",
       "mean   126092.336216\n",
       "std     11414.148432\n",
       "min     99542.000000\n",
       "25%    117191.250000\n",
       "50%    127012.500000\n",
       "75%    136056.750000\n",
       "max    143245.000000"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([len(i) for i in encoded_trace_df['dict index encoding']]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import tqdm\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['system calls frequency' ,'system calls tfidf','system calls hashing', 'system calls dependency graph', 'one hot encoding', 'dict index encoding' ]\n",
    "malwares=[\"delay\", \"disorder\", \"freeze\", \"hop\", \"mimic\", \"noise\", \"repeat\", \"spoof\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = encoded_trace_df[encoded_trace_df.maltype=='normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_matrix(matrix_list):\n",
    "    new_list = [np.array(i).reshape(-1) for i in matrix_list]\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_onehot(onehot_list, padding):\n",
    "    new_list = []\n",
    "    for onehot in onehot_list:\n",
    "        if len(onehot) > padding:\n",
    "            onehot = np.array(onehot[0:padding])\n",
    "            new_list.append(onehot)\n",
    "        else:\n",
    "            onehot =np.pad(onehot, [(0, padding-len(onehot)), (0, 0)], mode='constant', constant_values=0)\n",
    "            new_list.append(onehot)\n",
    "    # new_list = reshape_matrix(new_list)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normal['one hot encoding'].tolist()\n",
    "y = np.zeros(len(X))\n",
    "X = padding_onehot(X, 170000)\n",
    "mlb = LabelBinarizer()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "X_train = [T.FloatTensor(i) for i in X_train]\n",
    "X_val = [T.FloatTensor(i) for i in X_val]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = T.utils.data.DataLoader(X_train, batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = T.utils.data.DataLoader(X_val, batch_size=256, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM_Encoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=8):\n",
    "        super(CNN_LSTM_Encoder, self).__init__()\n",
    "        self.seq_len, self.n_features = seq_len, n_features\n",
    "        self.embedding_dim, self.hidden_dim = embedding_dim, 4 * embedding_dim\n",
    "        self.cnn = nn.Conv1d(self.seq_len, 100, 1, 1)\n",
    "        self.rnn1 = nn.LSTM(\n",
    "        input_size=n_features,\n",
    "        hidden_size=self.hidden_dim,\n",
    "        num_layers=1,\n",
    "        batch_first=True\n",
    "        )\n",
    "        self.rnn2 = nn.LSTM(\n",
    "        input_size=self.hidden_dim,\n",
    "        hidden_size=embedding_dim,\n",
    "        num_layers=1,\n",
    "        batch_first=True\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x, (_, _) = self.rnn1(x)\n",
    "        x, (_, _) = self.rnn2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM_Decoder(nn.Module):\n",
    "    def __init__(self, seq_len, input_dim=8, n_features=1):\n",
    "        super(CNN_LSTM_Decoder, self).__init__()\n",
    "        self.seq_len, self.input_dim = seq_len, input_dim\n",
    "        self.hidden_dim, self.n_features = 4 * input_dim, n_features\n",
    "        self.cnn = nn.ConvTranspose1d(100,self.seq_len,1,1)\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.rnn2 = nn.LSTM(\n",
    "            input_size=self.hidden_dim,\n",
    "            hidden_size=self.n_features,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, (_, _) = self.rnn1(x)\n",
    "        x, (_, _) = self.rnn2(x)\n",
    "        x = self.cnn(x)\n",
    "        # x = x.reshape((self.seq_len, self.hidden_dim))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder_LSTM(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=4):\n",
    "        super(Autoencoder_LSTM, self).__init__()\n",
    "        self.encoder = CNN_LSTM_Encoder(seq_len, n_features, embedding_dim)\n",
    "        self.decoder = CNN_LSTM_Decoder(seq_len, embedding_dim, n_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, seq_len, n_features):\n",
    "    net = Autoencoder_LSTM(seq_len, n_features, 4)\n",
    "    net = net.train() \n",
    "    loss_func = T.nn.MSELoss()\n",
    "    optimizer = T.optim.Adam(net.parameters(), lr=0.01)\n",
    "    max_epochs = 200\n",
    "    print(\"Starting training\")\n",
    "    last_val_loss = 100000000\n",
    "    patience = 0\n",
    "    for epoch in range(0, max_epochs):\n",
    "        loss = 0\n",
    "        if epoch > 0 and epoch % (max_epochs/10) == 0:\n",
    "            print(\"epoch = %6d\" % epoch, end=\"\")\n",
    "            print(\"  prev batch loss = %7.4f, perv batch val-loss = %7.4f\" %( loss/len(train_loader),val_loss/len(val_loader)))\n",
    "        for curr_bat in train_loader:\n",
    "            X = T.Tensor(curr_bat)\n",
    "            optimizer.zero_grad()\n",
    "            oupt = net(X)\n",
    "            loss_obj = loss_func(oupt, X)  # note X not Y\n",
    "            loss += loss_obj\n",
    "            loss_obj.backward()\n",
    "            optimizer.step()\n",
    "        val_loss = 0\n",
    "        with T.no_grad():\n",
    "            for curr_bat in val_loader:\n",
    "                X = T.Tensor(curr_bat)\n",
    "                oupt = net(X)\n",
    "                val_loss_obj = loss_func(oupt, X)  # note X not Y\n",
    "                val_loss += val_loss_obj.item()\n",
    "            if val_loss < last_val_loss:\n",
    "                last_val_loss = val_loss\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "        if patience >= 20:\n",
    "            break               \n",
    "    print(\"Training complete\")\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(net, train_loader):\n",
    "    net = net.eval()\n",
    "    pred = []\n",
    "    x_train = []\n",
    "    with T.no_grad():\n",
    "        for x_t in train_loader:\n",
    "            y_t = net(x_t)\n",
    "            x_train.append(x_t)\n",
    "            pred.append(y_t)\n",
    "        flat_x = np.concatenate(x_train,)\n",
    "        flat_pred_outs = np.concatenate(pred,)\n",
    "        y_pred = np.array([float(T.sum((T.FloatTensor(flat_x[i])-T.FloatTensor(flat_pred_outs[i]))*(T.FloatTensor(flat_x[i])-T.FloatTensor(flat_pred_outs[i])))) for i in range(0,len(flat_x))])\n",
    "        down_threshold = np.percentile(y_pred, 5)\n",
    "        up_threshold = np.percentile(y_pred, 95)\n",
    "    return down_threshold, up_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, test_loader, down_threshold, up_threshold):\n",
    "    net = net.eval()\n",
    "    pred = []\n",
    "    x_test = []\n",
    "    y_pred = []\n",
    "    with T.no_grad():\n",
    "        for x_t in test_loader:\n",
    "            y_t = net(x_t)\n",
    "            x_test.append(x_t)\n",
    "            pred.append(y_t)\n",
    "        flat_x = np.concatenate(x_test,)\n",
    "        flat_pred_outs = np.concatenate(pred,)\n",
    "        for i in range(0,len(flat_x)):\n",
    "            t = T.sum((T.FloatTensor(flat_x[i])-T.FloatTensor(flat_pred_outs[i]))*(T.FloatTensor(flat_x[i])-T.FloatTensor(flat_pred_outs[i])))\n",
    "            if t > down_threshold and t < up_threshold:\n",
    "                y_pred.append(0)\n",
    "            else:\n",
    "                y_pred.append(1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsdict = dict()\n",
    "predsdict = dict()\n",
    "classifiersdict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "epoch =     20  prev batch loss =  0.0000, perv batch val-loss =  0.0384\n",
      "epoch =     40  prev batch loss =  0.0000, perv batch val-loss =  0.0384\n",
      "Training complete\n",
      "Model: valid_one hot encoding_Autoencoder_RNN, accuracy score: 0.8991596638655462, training time is: 1287.4521706104279 seconds\n",
      "Model: delay_one hot encoding_Autoencoder_RNN, accuracy score: 0.22010869565217392, testing time is: 20.770519495010376 seconds\n",
      "Model: disorder_one hot encoding_Autoencoder_RNN, accuracy score: 0.1590909090909091, testing time is: 21.942049741744995 seconds\n",
      "Model: freeze_one hot encoding_Autoencoder_RNN, accuracy score: 0.1893939393939394, testing time is: 22.01976203918457 seconds\n",
      "Model: hop_one hot encoding_Autoencoder_RNN, accuracy score: 0.13131313131313133, testing time is: 22.31789207458496 seconds\n",
      "Model: mimic_one hot encoding_Autoencoder_RNN, accuracy score: 0.18686868686868688, testing time is: 21.838648080825806 seconds\n",
      "Model: noise_one hot encoding_Autoencoder_RNN, accuracy score: 0.13636363636363635, testing time is: 21.995513677597046 seconds\n",
      "Model: repeat_one hot encoding_Autoencoder_RNN, accuracy score: 0.1691919191919192, testing time is: 17.552650690078735 seconds\n",
      "Model: spoof_one hot encoding_Autoencoder_RNN, accuracy score: 0.14646464646464646, testing time is: 17.15799331665039 seconds\n"
     ]
    }
   ],
   "source": [
    "# features = ['dependency_graph_features', 'one_hot_features_50', 'one_hot_features_100', 'one_hot_features_200', 'one_hot_features_500',  'one_hot_features_1000', 'one_hot_features_2000']\n",
    "features = ['one hot encoding']\n",
    "for feature in features:\n",
    "    if 'one hot' in feature :\n",
    "        X = normal['one hot encoding'].tolist()\n",
    "        padding = 140000\n",
    "        seq_len = padding\n",
    "        X = padding_onehot(X, padding)\n",
    "    else:\n",
    "         X = normal[feature].tolist()\n",
    "         seq_len = 15\n",
    "    y = np.zeros(len(X))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "    X_train = [T.FloatTensor(i) for i in X_train]\n",
    "    X_val = [T.FloatTensor(i) for i in X_val]\n",
    "    train_loader = T.utils.data.DataLoader(X_train, batch_size=256, shuffle=False, drop_last=False, pin_memory=True, num_workers=4)\n",
    "    val_loader = T.utils.data.DataLoader(X_val, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "    #train stage\n",
    "    n_features = 15\n",
    "    t1 = time.time()\n",
    "    net = train(train_loader,val_loader, seq_len, n_features)\n",
    "    t2 = time.time()\n",
    "    \n",
    "    t = t2 -t1\n",
    "\n",
    "    # validation stage\n",
    "    down_threshold, up_threshold = find_threshold(net, train_loader)\n",
    "    y_pred = test(net, val_loader, down_threshold, up_threshold)\n",
    "    score = metrics.accuracy_score(y_val,y_pred)\n",
    "    name = 'Autoencoder_RNN'\n",
    "    res = dict()\n",
    "    res['Model'] = 'valid_' + feature + '_' + name\n",
    "    res['Accuracy'] = score\n",
    "    res['Training time'] = t\n",
    "\n",
    "    classifier = dict()\n",
    "    classifier['Autoencoder_RNN'] = net\n",
    "    classifier['down_threshold'] = down_threshold \n",
    "    classifier['up_threshold'] = up_threshold \n",
    "    print('Model: {}, accuracy score: {}, training time is: {} seconds'.format(res['Model'], score, t))\n",
    "\n",
    "    classifiersdict[res['Model']] = classifier\n",
    "    resultsdict[res['Model']] = res\n",
    "    predsdict[res['Model']] = y_pred\n",
    "    \n",
    "    # testing stage\n",
    "    for malware in malwares:\n",
    "        dfs = encoded_trace_df[encoded_trace_df.maltype==malware]\n",
    "        if 'one hot' in feature :\n",
    "            X = dfs['one hot encoding'].tolist()\n",
    "            padding = 140000\n",
    "            seq_len = padding\n",
    "            X = padding_onehot(X, padding)\n",
    "        else:\n",
    "            X = dfs[feature].tolist()\n",
    "            seq_len = 15\n",
    "        mlb = LabelBinarizer()\n",
    "        y_test = np.ones(len(X))\n",
    "        X_test = [T.FloatTensor(i) for i in X]\n",
    "        \n",
    "        test_loader =  T.utils.data.DataLoader(X_test, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "        t1 =time.time()\n",
    "        y_pred = test(net, test_loader, down_threshold, up_threshold)\n",
    "        t2 =time.time()\n",
    "        score = metrics.accuracy_score(y_test,y_pred)\n",
    "        t = t2 -t1\n",
    "        pred = dict()    \n",
    "        name = 'Autoencoder_RNN'\n",
    "        res = dict()\n",
    "\n",
    "        pred[malware +'_' + feature + '_' + name] = y_pred\n",
    "\n",
    "        res['Model'] = malware +'_' + feature + '_' + name\n",
    "        res['Accuracy'] = score\n",
    "        res['Testing time'] = t\n",
    "\n",
    "\n",
    "        resultsdict[res['Model']] = res\n",
    "        predsdict[res['Model']] = y_pred   \n",
    "        print('Model: {}, accuracy score: {}, testing time is: {} seconds'.format( res['Model'], score, t))\n",
    "        \n",
    "loc=open(resultsPath+'rnn_classifiers.pk','wb')\n",
    "pickle.dump(classifiersdict,loc)\n",
    "loc=open(resultsPath+'rnn_results.pk','wb')\n",
    "pickle.dump(resultsdict,loc)\n",
    "loc=open(resultsPath+'rnn_preds.pk','wb')\n",
    "pickle.dump(predsdict,loc)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = []\n",
    "for m in resultsdict:\n",
    "    rd.append(resultsdict[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrd = pd.DataFrame(rd, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = [i.split('_') for i in rrd['Model']]\n",
    "md = pd.DataFrame(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.columns  = ['Dataset','Features','Model','Architecture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrd=pd.DataFrame([md['Dataset'],md['Features'],md['Model'],md['Architecture'], rrd['Accuracy']]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrd.to_csv(resultsPath+'Autoencoder_RNN_results.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['1d 2sd 3f 4f', '1d 4f 3f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = TfidfVectorizer().fit_transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44832087, 0.44832087, 0.63009934, 0.44832087, 0.57735027,\n",
       "       0.57735027, 0.57735027])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x4 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft[0][0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
