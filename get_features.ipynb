{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "import os,sys\n",
    "import tqdm\n",
    "import pickle\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor  \n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dicts(base_dict_path, tw):\n",
    "    vectorizers = {}\n",
    "    dicts ={}\n",
    "    dictPath = base_dict_path + str(tw) + '/'\n",
    "    for i in range(1, 6):\n",
    "        cvName = 'countvectorizer_ngram{}'.format(i)\n",
    "        tvName = 'tfidfvectorizer_ngram{}'.format(i)\n",
    "        hvName = 'hashingvectorizer_ngram{}'.format(i)\n",
    "        ndName = 'ngrams_dict_ngram{}'.format(i)\n",
    "        sdName = 'syscall_dict_ngram{}'.format(i)\n",
    "        shdName = 'syscall_dict_onehot_ngram{}'.format(i)\n",
    "\n",
    "        loc=open(dictPath + cvName+'.pk','rb')\n",
    "        cv = pickle.load(loc)\n",
    "        vectorizers[cvName] = cv\n",
    "\n",
    "        loc=open(dictPath + tvName+'.pk','rb')\n",
    "        tv = pickle.load(loc)\n",
    "        vectorizers[tvName] = tv\n",
    "\n",
    "        loc=open(dictPath + hvName+'.pk','rb')\n",
    "        hv = pickle.load(loc)\n",
    "        vectorizers[hvName] = hv\n",
    "\n",
    "        loc=open(dictPath + ndName+'.pk','rb')\n",
    "        nd = pickle.load(loc)\n",
    "        dicts[ndName] = nd\n",
    "\n",
    "        loc=open(dictPath + sdName+'.pk','rb')\n",
    "        sd = pickle.load(loc)\n",
    "        dicts[sdName] = sd\n",
    "\n",
    "        loc=open(dictPath + shdName+'.pk','rb')\n",
    "        shd = pickle.load(loc)\n",
    "        dicts[shdName] = shd\n",
    "    \n",
    "    return vectorizers, dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootPath ='D:/git/IoT_Sensors_Security_Analysis/data/perf/'\n",
    "rawdataPath =rootPath+'{}/splited_1/{}/'.format('pi3', 60)\n",
    "rawFileNames = os.listdir(rawdataPath)\n",
    "rawdatas = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_trace_to_longstr(syscall_trace):\n",
    "    tracestr = ''\n",
    "    for syscall in syscall_trace:\n",
    "        tracestr += syscall + ' '\n",
    "    # print(tracestr)\n",
    "    return tracestr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_rawdata( rawdataPath, rawFileNames):    \n",
    "    corpus_dataframe, corpus = [],[]\n",
    "    par = tqdm.tqdm(total=len(rawFileNames), ncols=100)\n",
    "    for fn in rawFileNames:\n",
    "        if '.csv' in fn:\n",
    "            par.update(1)\n",
    "            fp = rawdataPath + fn\n",
    "            trace = pd.read_csv(fp)\n",
    "            tr = trace['syscall'].tolist()             \n",
    "            longstr = from_trace_to_longstr(tr)\n",
    "            corpus_dataframe.append(trace)\n",
    "            corpus.append(longstr)\n",
    "    par.close()\n",
    "    return corpus_dataframe, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                             | 7/3219 [00:06<52:50,  1.01it/s]C:\\Users\\luke-\\AppData\\Local\\Temp/ipykernel_17184/3939697684.py:1: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  corpus_dataframe, corpus = read_all_rawdata( rawdataPath, rawFileNames)\n",
      "  0%|▏                                                            | 12/3219 [00:10<36:23,  1.47it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17184/3939697684.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus_dataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_all_rawdata\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mrawdataPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawFileNames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17184/553874756.py\u001b[0m in \u001b[0;36mread_all_rawdata\u001b[1;34m(rawdataPath, rawFileNames)\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mpar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrawdataPath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mtrace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mtr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'syscall'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mlongstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrom_trace_to_longstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1421\u001b[0m     \"\"\"\n\u001b[0;32m   1422\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus_dataframe, corpus = read_all_rawdata( rawdataPath, rawFileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syscall_dict(ngrams_dict):\n",
    "    syscall_dict = {}\n",
    "    i = 0\n",
    "    for ngram in ngrams_dict:\n",
    "        if len(ngram.split()) == 1:\n",
    "            syscall_dict[ngram] = i\n",
    "            i+=1\n",
    "    return syscall_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizers(corpus, ngram):\n",
    "    syscall_dict = {}\n",
    "    ngrams_dict = {}\n",
    "    # countvectorizer = CountVectorizer().fit(corpus)\n",
    "    # syscall_dict = countvectorizer.vocabulary_\n",
    "    countvectorizer = CountVectorizer(ngram_range=(1, ngram)).fit(corpus)\n",
    "    print('create count vectorizer finished')\n",
    "    ngrams_dict = countvectorizer.vocabulary_\n",
    "    syscall_dict = get_syscall_dict(ngrams_dict)\n",
    "    tfidfvectorizer = TfidfVectorizer(ngram_range=(1, ngram), vocabulary=ngrams_dict).fit(corpus)\n",
    "    print('create tf-idf vectorizer finished')\n",
    "    hashingvectorizer = HashingVectorizer(n_features=2**5).fit(corpus)  \n",
    "    print('create hashing vectorizer finished')\n",
    "    return syscall_dict, ngrams_dict, countvectorizer, tfidfvectorizer, hashingvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_rawdata(corpus_dataframe,corpus, rawdataPath, rawFileNames):    \n",
    "    pool = ThreadPoolExecutor  (max_workers = 16)\n",
    "    def read_file(inputFilePath):\n",
    "        trace = pd.read_csv(inputFilePath)\n",
    "        tr = trace['syscall'].tolist()             \n",
    "        longstr = from_trace_to_longstr(tr)\n",
    "        return (trace,longstr)\n",
    "        # print(inputFilePath)\n",
    "    def asyn_page(filenames):\n",
    "        future_to_url  = dict()\n",
    "        for i, url in enumerate(filenames):\n",
    "            t = pool.submit(read_file, url)\n",
    "            future_to_url[t] = url               \n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                (trace,longstr) = data\n",
    "                corpus_dataframe.append(trace)\n",
    "                corpus.append(longstr)\n",
    "            except Exception as exc:\n",
    "                print('%r generated an exception: %s' % (filenames, exc))\n",
    "    \n",
    "    par = tqdm.tqdm(total = len(rawFileNames), ncols=100)\n",
    "    # i = 0\n",
    "    start, end = 0,0\n",
    "    for n in range(0,len(rawFileNames),16):\n",
    "        par.update(16)\n",
    "        start = n\n",
    "        if start + 16 < len(rawFileNames):\n",
    "            end = start + 16\n",
    "        else:\n",
    "            end = len(rawFileNames)\n",
    "        filenames = [rawdataPath + rawFileNames[i] for i in range(start, end)]\n",
    "        asyn_page(filenames)\n",
    "    par.close()\n",
    "    pool.shutdown()\n",
    "    print(\"Sub-process(es) done.\")\n",
    "    return corpus_dataframe, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onehot_encoding(total, index):\n",
    "    onehot = []\n",
    "    for i in range(0, total):\n",
    "        if i == index:\n",
    "            onehot.append(1)\n",
    "        else:\n",
    "            onehot.append(0)\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unk_to_dict(syscall_dict):\n",
    "    total = len(syscall_dict)\n",
    "    syscall_dict['unk'] = total\n",
    "    syscall_dict_onehot = dict()\n",
    "    for sc in syscall_dict:\n",
    "        syscall_dict_onehot[sc] = create_onehot_encoding(total+1, syscall_dict[sc])\n",
    "    return syscall_dict, syscall_dict_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_unk(syscall_trace, syscall_dict):\n",
    "    for i, sc in enumerate(syscall_trace):\n",
    "        if sc.lower() not in syscall_dict:\n",
    "            syscall_trace[i] = 'unk'\n",
    "    return syscall_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_onehot_encoding(trace, syscall_dict_onehot):\n",
    "    encoded_trace = []\n",
    "    for syscall in trace:\n",
    "        syscall = syscall.lower()\n",
    "        if syscall.lower() in syscall_dict_onehot:\n",
    "            one_hot = syscall_dict_onehot[syscall]\n",
    "        else:\n",
    "            syscall = 'UNK'\n",
    "            one_hot = syscall_dict_onehot[syscall]\n",
    "        encoded_trace.append(one_hot)\n",
    "    return encoded_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_head(trace, head):\n",
    "    starts, ends,se = [], [], []\n",
    "\n",
    "    for i,s in enumerate(trace):\n",
    "        if s == head:\n",
    "            start=i\n",
    "            starts.append(start)\n",
    "            if len(starts) > 1:\n",
    "                end = starts[-1] \n",
    "                ends.append(end)\n",
    "        if i == len(trace)-1:\n",
    "            end = len(trace)\n",
    "            ends.append(end)\n",
    "    se = [(starts[i], ends[i]) for i in range(0, len(starts))]\n",
    "    return se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance( trace, head, tails):\n",
    "    se = find_all_head(trace, head)\n",
    "    mw = 12\n",
    "    pool = ThreadPoolExecutor  (max_workers = mw)\n",
    "    distances = []\n",
    "    res = dict()\n",
    "    def return_distance(tails, sort):\n",
    "        distance = dict()\n",
    "        for tail in tails:\n",
    "            d = 0\n",
    "            for j,t in enumerate(sort):\n",
    "                if t==tail:\n",
    "                    d += 1/(j)\n",
    "            distance[tail] = d\n",
    "        return distance\n",
    "\n",
    "    def asyn_page(tails, sorts):\n",
    "        future_to_url  = dict()\n",
    "        for i, url in enumerate(sorts):\n",
    "            t = pool.submit(return_distance, tails=tails, sort=url)\n",
    "            future_to_url[t] = url               \n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                distance = data\n",
    "                distances.append(distance)\n",
    "            except Exception as exc:\n",
    "                print('generated an exception: %s' % (exc))\n",
    "    start, end = 0,0\n",
    "    for n in range(0, len(se), mw):\n",
    "        start = n\n",
    "        if start + mw < len(trace):\n",
    "            end = start + mw\n",
    "        else:\n",
    "            end = len(trace)            \n",
    "        sorts = [trace[s:e] for (s, e) in se[start:end]]\n",
    "        asyn_page(tails, sorts)\n",
    "    pool.shutdown()\n",
    "    # print(\"Sub-process(es) done.\")\n",
    "    ds = pd.DataFrame(distances)\n",
    "    for tail in tails:\n",
    "        if tail in ds:\n",
    "            res[(head, tail)] = sum(ds[tail])\n",
    "        else:\n",
    "            res[(head, tail)] = 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependency_graph( trace, term_dict):\n",
    "    dp = []\n",
    "    dp_ = {}\n",
    "    for head in term_dict:\n",
    "        tails = list(term_dict.keys())\n",
    "        tails.remove(head)  \n",
    "        p = get_distance( trace, head,tails)\n",
    "        dp_ = {**dp_, **p}\n",
    "    \n",
    "    for head in term_dict:\n",
    "        d_ =  []\n",
    "        for tail in term_dict:\n",
    "            if head == tail:\n",
    "                d_.append(0)\n",
    "            else:\n",
    "                d_.append(dp_[(head, tail)])\n",
    "        dp.append(d_)\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_sequence(trace,term_dict):\n",
    "    dict_sequence = []\n",
    "    for syscall in trace:\n",
    "        if syscall in term_dict:\n",
    "            dict_sequence.append(term_dict[syscall])\n",
    "        else:\n",
    "            dict_sequence.append(term_dict['unk'])\n",
    "    return dict_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_features(rawFileNames, corpus_dataframe, syscall_dict, syscall_dict_onehot):\n",
    "    one_hot_features = []\n",
    "    dependency_graph_features = []\n",
    "    dict_sequence_features = []\n",
    "    pool = ThreadPoolExecutor  (max_workers = 16)\n",
    "\n",
    "    def get_features(trace, syscall_dict, syscall_dict_onehot):\n",
    "        syscall_one_hot = []\n",
    "        dependency_graph = []\n",
    "        dict_sequence = []\n",
    "        syscall_trace = replace_with_unk(trace['syscall'].to_list(), syscall_dict)\n",
    "        syscall_one_hot =  trace_onehot_encoding(syscall_trace, syscall_dict_onehot)\n",
    "        dependency_graph = get_dependency_graph(rawFileNames, syscall_trace,syscall_dict)\n",
    "        dict_sequence=get_dict_sequence(syscall_trace,syscall_dict)\n",
    "        return (syscall_one_hot, dependency_graph, dict_sequence)\n",
    "\n",
    "    def asyn_page(cf):\n",
    "        future_to_url  = dict()\n",
    "        for i, url in enumerate(cf):\n",
    "            t = pool.submit(get_features, url, syscall_dict, syscall_dict_onehot)\n",
    "            future_to_url[t] = url               \n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                (syscall_one_hot, dependency_graph, dict_sequence) = data\n",
    "                one_hot_features.append(syscall_one_hot)\n",
    "                dependency_graph_features.append(dependency_graph)\n",
    "                dict_sequence_features.append(dict_sequence)\n",
    "            except Exception as exc:\n",
    "                print('generated an exception: %s' % ( exc))\n",
    "\n",
    "\n",
    "    par = tqdm.tqdm(total = len(corpus_dataframe), ncols=100)\n",
    "    # i = 0\n",
    "    start, end = 0,0\n",
    "    for n in range(0,len(corpus_dataframe),16):\n",
    "        par.update(16)\n",
    "        start = n\n",
    "        if start + 16 < len(corpus_dataframe):\n",
    "            end = start + 16\n",
    "        else:\n",
    "            end = len(corpus_dataframe)\n",
    "        cf = corpus_dataframe[start: end]\n",
    "        asyn_page(cf)\n",
    "    par.close()   \n",
    "    return one_hot_features, dependency_graph_features, dict_sequence_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootPath ='D:/git/IoT_Sensors_Security_Analysis/data/perf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(device, tw):\n",
    "    rawdataPath =rootPath+'{}/splited_1/{}/'.format(device, tw)\n",
    "    rawFileNames = os.listdir(rawdataPath)\n",
    "    rawdatas = dict()\n",
    "    for n in range(0,len(rawFileNames),1000):\n",
    "        start = n\n",
    "        if start + 1000 < len(rawFileNames):\n",
    "            end = start + 1000\n",
    "        else:\n",
    "            end = len(rawFileNames)\n",
    "        rawdatas[n] = rawFileNames[start:end]\n",
    "    corpus_dataframes, corpuses = {}, {}\n",
    "\n",
    "    for rfs in rawdatas:\n",
    "        corpus_dataframe, corpus = [],[]\n",
    "        corpus_dataframe, corpus = read_all_rawdata(corpus_dataframe,corpus, rawdataPath, rawdatas[rfs])\n",
    "        corpus_dataframes[rfs] = corpus_dataframe\n",
    "        corpuses[rfs] = corpus\n",
    "    \n",
    "    corpus_dataframe, corpus = [],[]\n",
    "    for rfs in corpus_dataframes:\n",
    "        corpus_dataframe += corpus_dataframes[rfs]\n",
    "        corpus += corpuses[rfs]\n",
    "    \n",
    "    syscall_dict, ngrams_dict, countvectorizer, tfidfvectorizer, hashingvectorizer = create_vectorizers(corpus, 3)\n",
    "    syscall_dict, syscall_dict_onehot = add_unk_to_dict(syscall_dict)\n",
    "\n",
    "    frequency_features = countvectorizer.transform(corpus)\n",
    "    tfidf_features = tfidfvectorizer.transform(corpus)\n",
    "    hashing_features = hashingvectorizer.transform(corpus)\n",
    "    one_hot_features, dependency_graph_features, dict_sequence_features = get_sequence_features(rawFileNames, corpus_dataframe, syscall_dict, syscall_dict_onehot)\n",
    "\n",
    "    maltype = []\n",
    "    ids = []\n",
    "    \n",
    "    for fi in rawFileNames:\n",
    "        fis = fi.split('_')\n",
    "        fn = fis[0]\n",
    "        i = '{}_{}_{}'.format(fis[0], fis[2], fis[3])\n",
    "        maltype.append(fn)\n",
    "        ids.append(i)\n",
    "    \n",
    "    dictPath = rootPath +'dicts/{}/{}/'.format(device, tw)\n",
    "    loc=open(dictPath+'countvectorizer.pk','wb')\n",
    "    pickle.dump(countvectorizer,loc)\n",
    "    loc=open(dictPath+'tfidfvectorizer.pk','wb')\n",
    "    pickle.dump(tfidfvectorizer,loc)\n",
    "    loc=open(dictPath+'hashingvectorizer.pk','wb')\n",
    "    pickle.dump(hashingvectorizer,loc)\n",
    "    loc=open(dictPath+'syscall_dict.pk','wb')\n",
    "    pickle.dump(syscall_dict,loc)\n",
    "    loc=open(dictPath+'syscall_dict_onehot.pk','wb')\n",
    "    pickle.dump(syscall_dict_onehot,loc)\n",
    "    loc=open(dictPath+'ngrams_dict.pk','wb')\n",
    "    pickle.dump(ngrams_dict,loc)\n",
    "    loc.close()\n",
    "\n",
    "    encoded_trace_df = pd.DataFrame([ids, maltype,frequency_features.toarray() ,tfidf_features.toarray(),hashing_features.toarray(), dependency_graph_features, one_hot_features, dict_sequence_features] ).transpose()\n",
    "    encoded_trace_df.columns = ['ids', 'maltype',  'system calls frequency' ,'system calls tfidf','system calls hashing', 'system calls dependency graph', 'one hot encoding', 'dict index encoding']\n",
    "    resultsPath = 'D:/git/IoT_Sensors_Security_Analysis/results/{}/tw_{}_turn_1/'.format(device, tw)\n",
    "    encoded_trace_df.to_pickle(resultsPath+'encoded_bow.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_matrix(matrix_list):\n",
    "    new_list = [np.array(i).reshape(-1) for i in matrix_list]\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_onehot(onehot_list, padding):\n",
    "    new_list = []\n",
    "    for onehot in onehot_list:\n",
    "        if len(onehot) > padding:\n",
    "            onehot = np.array(onehot[0:padding])\n",
    "            new_list.append(onehot)\n",
    "        else:\n",
    "            onehot =np.pad(onehot, [(0, padding-len(onehot)), (0, 0)], mode='constant', constant_values=0)\n",
    "            new_list.append(onehot)\n",
    "    new_list = reshape_matrix(new_list)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_dictencoding(dictencoding_list, padding):\n",
    "    new_list = []\n",
    "    for onehot in dictencoding_list:\n",
    "        if len(onehot) > padding:\n",
    "            onehot = np.array(onehot[0:padding])\n",
    "            new_list.append(onehot)\n",
    "        else:\n",
    "            onehot =np.pad(onehot, [(0, padding-len(onehot))], mode='constant', constant_values=0)\n",
    "            new_list.append(onehot)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_feature(input):\n",
    "    pca = PCA(n_components=100)\n",
    "    X_pca = pca.fit_transform(input)\n",
    "    return X_pca, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    rawFileNames = os.listdir(rawdataPath)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def read_dfs(devices, tws):\n",
    "    for tw in tws:\n",
    "        for device in devices:\n",
    "            # rawdataPath =rootPath+'{}/splited_1/{}/'.format(device, tw)\n",
    "             rawdataPath =rootPath+'{}/'.format(device)\n",
    "            rawFileNames = os.listdir(rawdataPath)\n",
    "            rawdatas = dict()\n",
    "            for n in range(0,len(rawFileNames),1000):\n",
    "                start = n\n",
    "                if start + 1000 < len(rawFileNames):\n",
    "                    end = start + 1000\n",
    "                else:\n",
    "                    end = len(rawFileNames)\n",
    "                rawdatas[n] = rawFileNames[start:end]\n",
    "            corpus_dataframes, corpuses = {}, {}\n",
    "\n",
    "            for rfs in rawdatas:\n",
    "                corpus_dataframe, corpus = [],[]\n",
    "                corpus_dataframe, corpus = read_all_rawdata(corpus_dataframe,corpus, rawdataPath, rawdatas[rfs])\n",
    "                corpus_dataframes[rfs] = corpus_dataframe\n",
    "                corpuses[rfs] = corpus\n",
    "            \n",
    "            corpus_dataframe, corpus = [],[]\n",
    "            for rfs in corpus_dataframes:\n",
    "                corpus_dataframe += corpus_dataframes[rfs]\n",
    "                corpus += corpuses[rfs]\n",
    "\n",
    "            loc=open(rawdataPath+'corpus_dataframe.pk','wb')\n",
    "            pickle.dump(corpus_dataframe,loc)\n",
    "            loc=open(rawdataPath+'corpus.pk','wb')\n",
    "            pickle.dump(corpus,loc)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\concurrent\\futures\\thread.py:52: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  result = self.fn(*self.args, **self.kwargs)\n",
      "1008it [05:03,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1008it [03:30,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1008it [03:28,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1008it [05:08,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1008it [03:30,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1008it [03:27,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n"
     ]
    }
   ],
   "source": [
    "devices = [ 'pi4_2G', 'pi4_4G']\n",
    "tws=[60]\n",
    "read_dfs(devices, tws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootPath = 'd:/mt_data/1126_withrw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(devices, tws):\n",
    "    times = {} \n",
    "    # pcas = {}\n",
    "    for tw in tws:\n",
    "        base_dict_path = rootPath\n",
    "        vectorizers, dicts = read_dicts(base_dict_path, tw)\n",
    "\n",
    "        for device in devices:\n",
    "            features = []\n",
    "            rawdataPath =rootPath+'{}/'.format(device)\n",
    "            rawFileNames = os.listdir(rawdataPath)\n",
    "            ids, maltype = [], []\n",
    "            corpus_dataframe, corpus=[], []\n",
    "            par = tqdm.tqdm(total=len(rawFileNames), ncols=80)\n",
    "            for fi in rawFileNames:    \n",
    "                par.update(1)            \n",
    "                if '.csv' or '.txt' in fi:\n",
    "                    fis = fi.split('_')\n",
    "                    if('_v2') in fi:\n",
    "                        fn = fis[0] + fis[1]\n",
    "                    else:\n",
    "                        fn = fis[0]\n",
    "                    i = '{}_{}_{}'.format(fis[0], fis[2], fis[3])\n",
    "                    maltype.append(fn)\n",
    "                    ids.append(i)\n",
    "                fi = rawdataPath + fi\n",
    "                if '.txt' in fi:\n",
    "                    trace = pd.read_csv(fi)\n",
    "                elif '.csv'in fi:\n",
    "                    trace = pd.read_csv(fi, header=None)\n",
    "                    trace.columns = ['pid','timestamp','syscall','time_cost']\n",
    "                trace = trace.drop(len(trace)-1)\n",
    "                tr = trace['syscall'].tolist()             \n",
    "                longstr = from_trace_to_longstr(tr)\n",
    "                corpus_dataframe.append(trace)\n",
    "                corpus.append(longstr)\n",
    "            par.close()\n",
    "            features.append(ids)\n",
    "            features.append(maltype)\n",
    "\n",
    "            # loc=open(rawdataPath +'corpus_dataframe.pk','rb')\n",
    "            # corpus_dataframe = pickle.load(loc)\n",
    "            # loc=open(rawdataPath +'corpus.pk','rb')\n",
    "            # corpus = pickle.load(loc)\n",
    "            print('got rawdata')\n",
    "            ndName = 'ngrams_dict_ngram{}'.format(1)\n",
    "            sdName = 'syscall_dict_ngram{}'.format(1)\n",
    "            shdName = 'syscall_dict_onehot_ngram{}'.format(1)\n",
    "            nd = dicts[ndName]\n",
    "            sd = dicts[sdName]\n",
    "            shd = dicts[shdName]\n",
    "            \n",
    "            # one_hot_features = []\n",
    "            # dependency_graph_features = []\n",
    "            # dict_sequence_features = []\n",
    "\n",
    "            # t1 = time.time()\n",
    "            # par = tqdm.tqdm(total=len(corpus_dataframe), ncols=100)\n",
    "            # for trace in corpus_dataframe:\n",
    "            #     syscall_trace = replace_with_unk(trace['syscall'].to_list(), sd)\n",
    "            #     syscall_one_hot =  trace_onehot_encoding(syscall_trace, shd)\n",
    "            #     one_hot_features.append(syscall_one_hot)\n",
    "            #     par.update(1)\n",
    "            # t2 = time.time()\n",
    "            # par.close()\n",
    "            # key = 'syscall_one_hot'+'_'+device+'_'+str(tw)\n",
    "            # t = t2 - t1\n",
    "            # times[key] = t\n",
    "            # print(key+\": \"+str(t))\n",
    "            # pca_name = key+'_pca'\n",
    "            # inputs = padding_onehot(one_hot_features, 160000)\n",
    "            # one_hot_features_pca, pca = get_pca_feature(inputs)\n",
    "            # pcas[pca_name] = pca\n",
    "            # features.append(one_hot_features)\n",
    "            # features.append(one_hot_features_pca)\n",
    "            # par = tqdm.tqdm(total=len(corpus_dataframe), ncols=100)\n",
    "            # t1 = time.time()\n",
    "            # for trace in corpus_dataframe:\n",
    "            #     syscall_trace = replace_with_unk(trace['syscall'].to_list(), sd)\n",
    "            #     dict_sequence = get_dict_sequence(syscall_trace,sd)\n",
    "            #     dict_sequence_features.append(dict_sequence)\n",
    "            #     par.update(1)\n",
    "            # t2 = time.time()\n",
    "            # par.close()\n",
    "            # t = t2 - t1\n",
    "            # key = 'dict_sequence'+'_'+device+'_'+str(tw)\n",
    "            # times[key] = t\n",
    "            # print(key+\": \"+str(t))\n",
    "            # pca_name = key+'_pca'\n",
    "            # inputs = padding_dictencoding(dict_sequence_features, 160000)\n",
    "            # dict_sequence_pca, pca = get_pca_feature(inputs)\n",
    "            # pcas[pca_name] = pca\n",
    "            # features.append(dict_sequence_features)\n",
    "            # features.append(dict_sequence_pca)\n",
    "\n",
    "\n",
    "            # par = tqdm.tqdm(total=len(corpus_dataframe), ncols=100)\n",
    "            # t1 = time.time()    \n",
    "            # for trace in corpus_dataframe:\n",
    "            #     syscall_trace = replace_with_unk(trace['syscall'].to_list(), sd)\n",
    "            #     dependency_graph = get_dependency_graph(syscall_trace,sd)\n",
    "            #     dependency_graph_features.append(dependency_graph)\n",
    "            #     par.update(1)\n",
    "            # t2 = time.time()\n",
    "            # par.close()\n",
    "            # t = t2 - t1\n",
    "            # key = 'dict_sequence'+'_'+device+'_'+str(tw)\n",
    "            # times[key] = t   \n",
    "            # print(key+\": \"+str(t))\n",
    "            # features.append(dependency_graph_features)\n",
    "\n",
    "            for i in range(1, 6):\n",
    "                cvName = 'countvectorizer_ngram{}'.format(i)\n",
    "                tvName = 'tfidfvectorizer_ngram{}'.format(i)\n",
    "                hvName = 'hashingvectorizer_ngram{}'.format(i)             \n",
    "\n",
    "                cv = vectorizers[cvName]\n",
    "                tv = vectorizers[tvName]\n",
    "                hv = vectorizers[hvName]\n",
    "\n",
    "                t1 = time.time()\n",
    "                frequency_features = cv.transform(corpus)\n",
    "                t2 = time.time()\n",
    "                key = cvName+'_'+device+'_'+str(tw)\n",
    "                t = t2 - t1\n",
    "                times[key] = t\n",
    "                print(key+\": \"+str(t))\n",
    "                frequency_features = frequency_features.toarray()\n",
    "                # frequency_pca_name = key+'_pca'\n",
    "                # frequency_pca,pca = get_pca_feature(frequency_features)\n",
    "                # pcas[frequency_pca_name] = pca               \n",
    "\n",
    "                t1 = time.time()\n",
    "                tfidf_features = tv.transform(corpus)\n",
    "                t2 = time.time()\n",
    "                t = t2 - t1\n",
    "                key = tvName+'_'+device+'_'+str(tw)\n",
    "                times[key] = t\n",
    "                print(key+\": \"+str(t))\n",
    "                tfidf_features = tfidf_features.toarray()\n",
    "                # tfidf_pca_name = key+'_pca'\n",
    "                # tfidf_pca,pca = get_pca_feature(tfidf_features)\n",
    "                # pcas[tfidf_pca_name] = pca\n",
    "\n",
    "                t1 = time.time()\n",
    "                hashing_features = hv.transform(corpus)\n",
    "                t2 = time.time()\n",
    "                t = t2 - t1\n",
    "                key = hvName+'_'+device+'_'+str(tw)\n",
    "                times[key] = t\n",
    "                print(key+\": \"+str(t))\n",
    "                hashing_features = hashing_features.toarray()\n",
    "\n",
    "                features.append(frequency_features)\n",
    "                # features.append(frequency_pca)\n",
    "                features.append(tfidf_features)\n",
    "                # features.append(tfidf_pca)\n",
    "                features.append(hashing_features)           \n",
    "            \n",
    "            encoded_trace_df = pd.DataFrame(features).transpose()\n",
    "            encoded_trace_df.columns = ['ids', 'maltype',#'one hot encoding', 'dict index encoding', 'system calls dependency graph', \n",
    "            'system calls frequency_1gram', 'system calls tfidf_1gram', 'system calls hashing_1gram',\n",
    "            'system calls frequency_2gram', 'system calls tfidf_2gram', 'system calls hashing_2gram',\n",
    "            'system calls frequency_3gram', 'system calls tfidf_3gram', 'system calls hashing_3gram',\n",
    "            'system calls frequency_4gram', 'system calls tfidf_4gram', 'system calls hashing_4gram',\n",
    "            'system calls frequency_5gram', 'system calls tfidf_5gram', 'system calls hashing_5gram'\n",
    "            ]\n",
    "            \n",
    "            resultsPath = rootPath+'encoded/t1/'\n",
    "            encoded_trace_df.to_pickle(resultsPath+'encoded_bow{}_{}.pkl'.format(device, tw))  \n",
    "    return times         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = [ 'pi3', 'pi4_2G', 'pi4_4G']\n",
    "tws = [60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luke-\\AppData\\Local\\Temp/ipykernel_13308/2826415934.py:1: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  times =  get_features(devices, tws)\n"
     ]
    }
   ],
   "source": [
    "times =  get_features(devices, tws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPath = rootPath+'encoded/t1/'\n",
    "loc=open(resultsPath +'encoded_bowpi4_4G_60.pkl','rb')\n",
    "corpus_dataframe = pickle.load(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdataPath =rootPath+'{}/'.format('pi4_4G')\n",
    "rawFileNames = os.listdir(rawdataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, maltype = [], []\n",
    "for fi in rawFileNames:              \n",
    "    if '.csv' or '.txt' in fi:\n",
    "        fis = fi.split('_')\n",
    "        if('_v2') in fi:\n",
    "            fn = fis[0] + fis[1]\n",
    "        else:\n",
    "            fn = fis[0]\n",
    "        i = '{}_{}_{}'.format(fis[0], fis[2], fis[3])\n",
    "        maltype.append(fn)\n",
    "        ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dataframe['maltype'] = maltype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dataframe.to_pickle(resultsPath+'encoded_bow{}_{}.pkl'.format('pi4_4G', 60))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
