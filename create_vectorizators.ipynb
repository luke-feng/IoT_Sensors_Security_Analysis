{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "import os,sys\n",
    "import tqdm\n",
    "import pickle\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor  \n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syscall_dict(ngrams_dict):\n",
    "    syscall_dict = {}\n",
    "    i = 0\n",
    "    for ngram in ngrams_dict:\n",
    "        if len(ngram.split()) == 1:\n",
    "            syscall_dict[ngram] = i\n",
    "            i+=1\n",
    "    return syscall_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizers(corpus, ngram):\n",
    "    syscall_dict = {}\n",
    "    ngrams_dict = {}\n",
    "    # countvectorizer = CountVectorizer().fit(corpus)\n",
    "    # syscall_dict = countvectorizer.vocabulary_\n",
    "    t1 = time.time()\n",
    "    countvectorizer = CountVectorizer(ngram_range=(1, ngram)).fit(corpus)\n",
    "    t2 = time.time()\n",
    "    print('create ngram {} count vectorizer finished, fitting time is {}'.format(ngram, t2-t1))\n",
    "    ngrams_dict = countvectorizer.vocabulary_\n",
    "    syscall_dict = get_syscall_dict(ngrams_dict)\n",
    "    t1 = time.time()\n",
    "    tfidfvectorizer = TfidfVectorizer(ngram_range=(1, ngram), vocabulary=ngrams_dict).fit(corpus)\n",
    "    t2 = time.time()\n",
    "    print('create ngram {} tf-idf vectorizer finished, fitting time is {}'.format(ngram, t2-t1))\n",
    "\n",
    "    t1 = time.time()\n",
    "    hashingvectorizer = HashingVectorizer(n_features=2**5).fit(corpus)  \n",
    "    t2 = time.time()\n",
    "    print('create ngram {} hashing vectorizer finished, fitting time is {}'.format(ngram,t2-t1))\n",
    "    return syscall_dict, ngrams_dict, countvectorizer, tfidfvectorizer, hashingvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_trace_to_longstr(syscall_trace):\n",
    "    tracestr = ''\n",
    "    for syscall in syscall_trace:\n",
    "        tracestr += syscall + ' '\n",
    "    # print(tracestr)\n",
    "    return tracestr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rawdata(corpus_dataframe,corpus, rawdataPath, rawFileNames):    \n",
    "    pool = ThreadPoolExecutor  (max_workers = 16)\n",
    "    def read_file(inputFilePath):\n",
    "        if '.txt' in inputFilePath:\n",
    "            trace = pd.read_csv(inputFilePath)\n",
    "        elif '.csv'in inputFilePath:\n",
    "            trace = pd.read_csv(inputFilePath, header=None)\n",
    "            trace.columns = ['pid','timestamp','syscall','time_cost']\n",
    "        trace = trace.drop(len(trace)-1)\n",
    "        tr = trace['syscall'].tolist()             \n",
    "        longstr = from_trace_to_longstr(tr)\n",
    "        return (trace,longstr)\n",
    "        # print(inputFilePath)\n",
    "    def asyn_page(filenames):\n",
    "        future_to_url  = dict()\n",
    "        for i, url in enumerate(filenames):\n",
    "            t = pool.submit(read_file, url)\n",
    "            future_to_url[t] = url               \n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                (trace,longstr) = data\n",
    "                corpus_dataframe.append(trace)\n",
    "                corpus.append(longstr)\n",
    "            except Exception as exc:\n",
    "                print('%r generated an exception: %s' % (filenames, exc))\n",
    "    \n",
    "    par = tqdm.tqdm(total = len(rawFileNames), ncols=100)\n",
    "    # i = 0\n",
    "    start, end = 0,0\n",
    "    for n in range(0,len(rawFileNames),16):\n",
    "        par.update(16)\n",
    "        start = n\n",
    "        if start + 16 < len(rawFileNames):\n",
    "            end = start + 16\n",
    "        else:\n",
    "            end = len(rawFileNames)\n",
    "        filenames = [rawdataPath + rawFileNames[i] for i in range(start, end)]\n",
    "        asyn_page(filenames)\n",
    "    par.close()\n",
    "    pool.shutdown()\n",
    "    print(\"Sub-process(es) done.\")\n",
    "    return corpus_dataframe, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rawdata_1(corpus_dataframe,corpus, rawdataPath, rawFileNames):   \n",
    "    for rawFileName in rawFileNames:\n",
    "        inputFilePath = rawdataPath + rawFileName\n",
    "        if '.txt' in inputFilePath:\n",
    "            trace = pd.read_csv(inputFilePath)\n",
    "        elif '.csv'in inputFilePath:\n",
    "            trace = pd.read_csv(inputFilePath, header=None)\n",
    "            trace.columns = ['pid','timestamp','syscall','time_cost']\n",
    "        tr = trace['syscall'].tolist()             \n",
    "        longstr = from_trace_to_longstr(tr)\n",
    "        corpus_dataframe.append(trace)\n",
    "        corpus.append(longstr)\n",
    "    return corpus_dataframe, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onehot_encoding(total, index):\n",
    "    onehot = []\n",
    "    for i in range(0, total):\n",
    "        if i == index:\n",
    "            onehot.append(1)\n",
    "        else:\n",
    "            onehot.append(0)\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unk_to_dict(syscall_dict):\n",
    "    total = len(syscall_dict)\n",
    "    syscall_dict['unk'] = total\n",
    "    syscall_dict_onehot = dict()\n",
    "    for sc in syscall_dict:\n",
    "        syscall_dict_onehot[sc] = create_onehot_encoding(total+1, syscall_dict[sc])\n",
    "    return syscall_dict, syscall_dict_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_vectorizators(corpus, device, tw, n_gram):\n",
    "    syscall_dict, ngrams_dict, countvectorizer, tfidfvectorizer, hashingvectorizer = create_vectorizers(corpus, n_gram)\n",
    "    syscall_dict, syscall_dict_onehot = add_unk_to_dict(syscall_dict)\n",
    "\n",
    "    # dictPath = rootPath +'dicts/uniform/{}/'.format(tw)\n",
    "    dictPath = rootPath +'{}/'.format(tw)\n",
    "    loc=open(dictPath+'countvectorizer_ngram{}.pk'.format(n_gram),'wb')\n",
    "    pickle.dump(countvectorizer,loc)\n",
    "    loc=open(dictPath+'tfidfvectorizer_ngram{}.pk'.format(n_gram),'wb')\n",
    "    pickle.dump(tfidfvectorizer,loc)\n",
    "    loc=open(dictPath+'hashingvectorizer_ngram{}.pk'.format(n_gram),'wb')\n",
    "    pickle.dump(hashingvectorizer,loc)\n",
    "    loc=open(dictPath+'syscall_dict_ngram{}.pk'.format(n_gram),'wb')\n",
    "    pickle.dump(syscall_dict,loc)\n",
    "    loc=open(dictPath+'syscall_dict_onehot_ngram{}.pk'.format(n_gram),'wb')\n",
    "    pickle.dump(syscall_dict_onehot,loc)\n",
    "    loc=open(dictPath+'ngrams_dict_ngram{}.pk'.format(n_gram),'wb')\n",
    "    pickle.dump(ngrams_dict,loc)\n",
    "    loc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(device, tw, cf, co):\n",
    "    # rawdataPath =rootPath+'{}/splited_1/{}/'.format(device, tw)\n",
    "    rawdataPath =rootPath+'{}/'.format(device)\n",
    "    rawFileNames = os.listdir(rawdataPath)\n",
    "    rawdatas = dict()\n",
    "    for n in range(0,len(rawFileNames),1000):\n",
    "        start = n\n",
    "        if start + 1000 < len(rawFileNames):\n",
    "            end = start + 1000\n",
    "        else:\n",
    "            end = len(rawFileNames)\n",
    "        rawdatas[n] = rawFileNames[start:end]\n",
    "    corpus_dataframes, corpuses = {}, {}\n",
    "\n",
    "    for rfs in rawdatas:\n",
    "        corpus_dataframe, corpus = [],[]\n",
    "        corpus_dataframe, corpus = read_rawdata(corpus_dataframe,corpus, rawdataPath, rawdatas[rfs])\n",
    "        corpus_dataframes[rfs] = corpus_dataframe\n",
    "        corpuses[rfs] = corpus\n",
    "    \n",
    "\n",
    "    for rfs in corpus_dataframes:\n",
    "        cf += corpus_dataframes[rfs]\n",
    "        co += corpuses[rfs]\n",
    "    return cf, co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootPath ='D:/mt_data/1126_withrw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = ['pi3', 'pi4_2G', 'pi4_4G']\n",
    "tws = [60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/100 [00:00<?, ?it/s]C:\\Python39\\lib\\concurrent\\futures\\thread.py:52: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  result = self.fn(*self.args, **self.kwargs)\n",
      "112it [00:13,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1008it [03:12,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1008it [03:17,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1008it [03:24,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "608it [02:05,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1008it [03:40,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1008it [03:25,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1008it [03:37,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "608it [02:03,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-process(es) done.\n",
      "7300\n",
      "create ngram 1 count vectorizer finished, fitting time is 513.5494050979614\n",
      "create ngram 1 tf-idf vectorizer finished, fitting time is 522.5125312805176\n",
      "create ngram 1 hashing vectorizer finished, fitting time is 0.0\n",
      "create ngram 2 count vectorizer finished, fitting time is 974.5562417507172\n",
      "create ngram 2 tf-idf vectorizer finished, fitting time is 947.1551079750061\n",
      "create ngram 2 hashing vectorizer finished, fitting time is 0.0\n",
      "create ngram 3 count vectorizer finished, fitting time is 1458.9337661266327\n",
      "create ngram 3 tf-idf vectorizer finished, fitting time is 1463.871610879898\n",
      "create ngram 3 hashing vectorizer finished, fitting time is 0.0010001659393310547\n",
      "create ngram 4 count vectorizer finished, fitting time is 1840.4847145080566\n",
      "create ngram 4 tf-idf vectorizer finished, fitting time is 1829.791666984558\n",
      "create ngram 4 hashing vectorizer finished, fitting time is 0.0\n",
      "create ngram 5 count vectorizer finished, fitting time is 2332.824594974518\n",
      "create ngram 5 tf-idf vectorizer finished, fitting time is 2373.9772984981537\n",
      "create ngram 5 hashing vectorizer finished, fitting time is 0.0\n"
     ]
    }
   ],
   "source": [
    "for tw in tws:\n",
    "   cf, co = [],[]\n",
    "   for device in devices:\n",
    "      cf, co = get_data(device, tw, cf, co)\n",
    "   print(len(cf))\n",
    "   for n_gram in range(1,6):\n",
    "      fit_vectorizators(co, device, tw, n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictPath = rootPath +'{}/'.format(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc=open(dictPath+'syscall_dict_ngram{}.pk'.format(1),'rb')\n",
    "syscall_dict = pickle.load(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ioctl': 0,\n",
       " 'timerfd_settime': 1,\n",
       " 'poll': 2,\n",
       " 'getpid': 3,\n",
       " 'write': 4,\n",
       " 'futex': 5,\n",
       " 'open': 6,\n",
       " 'close': 7,\n",
       " 'read': 8,\n",
       " 'mkdir': 9,\n",
       " 'fstat64': 10,\n",
       " 'getdents': 11,\n",
       " 'unlink': 12,\n",
       " 'mprotect': 13,\n",
       " 'clock_gettime': 14,\n",
       " 'gettimeofday': 15,\n",
       " 'madvise': 16,\n",
       " 'unk': 17}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syscall_dict"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
